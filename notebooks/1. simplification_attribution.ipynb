{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59239cbb-a912-499c-b0f1-2fb9742217cb",
   "metadata": {},
   "source": [
    "# Explaining Facial Expression Recognition with Simplification and Feature Attribution\n",
    "# Notebook 1:  XAI for Affective Computing (SoSe2024)\n",
    "\n",
    "In this notebook you will attempt to generate explanations for predictions of two Facial Expression recognition models, one for tabular (structured) data extracted from images and one for raw image data, both trained using a subset of the [AffectNet dataset](http://mohammadmahoor.com/affectnet/). AffectNet is an image dataset of facial expressions in the wild, and is labeled with 8 facial expression categories: **Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger, and Contempt**. (Have a look at the paper for more details https://arxiv.org/abs/1708.03985). \n",
    "\n",
    "In **Part 1**, you will first explore local and global explanations on a tabular dataset of [Facial Action Units (FAUs)](https://imotions.com/blog/facial-action-coding-system/) by using the [LIME python package](https://github.com/marcotcr/lime). The dataset is comprised of FAUs that were extracted from the face images of AffectNet using [OpenFace2.0](https://github.com/TadasBaltrusaitis/OpenFace). This dataset is then used to train a XGBoost classifier (trained model is provided in the code below).\n",
    "\n",
    "In **Part 2**, you will generate local explanations for a Convolutional Neural Network (CNN) using both LIME and a Backpropagation based method, either Layerwise Relevance Propagation or GradCAM.  The CNN is already trained using the raw images of AffectNet (trained model is provided in the code below). A subset of the test data is provided for generating and evaluating the explantions.  **_For this part you can choose between either TASK 3a, Saliency Maps with iNNvestigate, or TASK 3.6, GradCAM._**\n",
    "\n",
    "To use this notebook, please make sure to go step by step through each of the cells review the code and comments along the way.\n",
    "\n",
    "See **README** to get started with the Notebook setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2944a-a7fb-4d96-b652-8a2fe4f432d2",
   "metadata": {},
   "source": [
    "## Part 0: Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39040a80-0531-4da1-a98a-bd59f87d6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295770a4-0087-4da4-80f6-15d5df6a18a9",
   "metadata": {},
   "source": [
    "**Import necessary libraries**\n",
    "\n",
    "(see README for necessary package installations if you receive a `module not found` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b037e5c-d2f7-4c98-968b-e57231811d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"         # don't use keras 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# import tensorflow for model loading\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import sklearn for processing data and results\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# import model loading function\n",
    "from model import cnn_model, create_bn_replacment\n",
    "\n",
    "# import plotting helper functions\n",
    "import utils\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc15c5-cfb7-4daf-bff5-493b4c55fbaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Explanations of Facial Action Units\n",
    "\n",
    "In this part, we will generate explanations for the XGBoost model trained using a dataset of Facial Action Units (as described in the notebook introduction).  \n",
    "\n",
    "First, let's load the data and the trained models. Then we will evaluate the model peformance, before we start with the explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74272a30-eb14-4b45-80cb-01cc5f789639",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592663b-e998-4208-b4e4-a638e1692b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data from training and evaluation\n",
    "train_csv = '../data/affectnet_aus/train_aus.csv'\n",
    "val_csv = '../data/affectnet_aus/val_aus.csv'\n",
    "\n",
    "# load training and validation data as pandas dataframeas\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_val = pd.read_csv(val_csv)\n",
    "\n",
    "# smaller dataset for explanations (same data as in Task 1)\n",
    "xai_csv = '../data/affectnet_aus/eval_aus.csv'\n",
    "df_xai = pd.read_csv(xai_csv)\n",
    "\n",
    "# get only the columns storing action units from the dataframe\n",
    "feature_names = [col for col in df_val if col.startswith('AU')]\n",
    "categorical_features = [feat for feat in feature_names if '_c' in feat]\n",
    "categorical_idxs = [i for i, feat in enumerate(feature_names) if '_c' in feat]\n",
    "\n",
    "class_names = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']  # same class labels as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b3bbf-7ae5-49ba-a2c3-0574d51e3da1",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# convert data from dataframe to Numpy arrays\n",
    "X_train = df_train.loc[:, feature_names]\n",
    "y_train = df_train['class']\n",
    "# set categorical features in dataframe for XGboost\n",
    "for col in categorical_features:\n",
    "    X_train[col] = X_train[col].astype(int).astype('category')\n",
    "\n",
    "X_test = df_val.loc[:, feature_names]\n",
    "y_test = df_val['class']\n",
    "# set categorical features in dataframe for XGboost\n",
    "for col in categorical_features:\n",
    "    X_test[col] = X_test[col].astype(int).astype('category')\n",
    "\n",
    "X_xai = df_xai.loc[:, feature_names]\n",
    "y_xai = df_xai['class']\n",
    "# set categorical features in dataframe for XGboost\n",
    "for col in categorical_features:\n",
    "    X_xai[col] = X_xai[col].astype(int).astype('category')\n",
    "\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)\n",
    "print('XAI', X_xai.shape, y_xai.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5bf93-089c-4cfb-9d08-e07921c28bb7",
   "metadata": {},
   "source": [
    "### Load pretrained XGBoost model\n",
    "And validate that it works.  \n",
    "The accuracy of the model in the training data should be around $92\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba7f8e1a202326",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "random_state = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37adba153566d02",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Train model from scratch\n",
    "# clf = XGBClassifier(booster='gbtree', enable_categorical=True, max_depth=15, eta=0.1, reg_lambda=30, random_state=random_state) \n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.score(X_train, y_train)\n",
    "# clf.save_model(\"../models/xgboost_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262e012-07bb-41a2-be42-55fc5c8837dd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "clf = XGBClassifier(booster='gbtree', enable_categorical=True, max_depth=15, eta=0.1, reg_lambda=30, random_state=random_state)\n",
    "clf.load_model(\"../models/xgboost_model.json\")\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e8d94-975b-4750-bc14-e8316119d95a",
   "metadata": {},
   "source": [
    "### Now evaluate on the test data\n",
    "Unfortunately, the accuracy is only $44\\%$ but this is still well above chance guessing which would be $1 / 8 * 100 = 12.5\\%$ accuracy (since there are 8 total classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8d46d-dbf9-447c-9f18-f88d001142a9",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# get model predictions\n",
    "y_test_preds = clf.predict(X_test)\n",
    "y_test_true = y_test\n",
    "\n",
    "# eval results\n",
    "print(classification_report(y_test_true, y_test_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4d15c-2baa-4c2d-b0c6-8964feff96f9",
   "metadata": {},
   "source": [
    "We can also review the confusion matrix to see where the model makes its mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018ad2b-4b76-4746-91bd-5226245e995a",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm_data = confusion_matrix(y_test_true, y_test_preds)\n",
    "cm = pd.DataFrame(cm_data, columns=class_names, index=class_names)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87745914-96cb-41d3-a050-b4cf41261b09",
   "metadata": {},
   "source": [
    "### Evaluate with XAI Data\n",
    "Here we calculate and evaluate predictions on a smaller `X_xai` dataset.  `X_xai`is a subset of the full test data (from above) and will be used throughout the rest of part 1.\n",
    "\n",
    "The accuracy should be around $40\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff6cb2-b96a-43b8-9df6-5a2f77ab177f",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get model predictions\n",
    "y_xai_preds = clf.predict(X_xai)\n",
    "y_xai_true = y_xai\n",
    "\n",
    "print(classification_report(y_xai_true, y_xai_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afe66c-ff89-4e89-a98c-506c701fa7cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Display confusion matrix for `X_xai` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9173f4e-8b76-4230-a84a-e8a5312773ae",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "cm_data = confusion_matrix(y_xai_true, y_xai_preds)\n",
    "cm = pd.DataFrame(cm_data, columns=class_names, index=class_names)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df582f5e-52e6-48b5-8fb8-6023b8b8bb51",
   "metadata": {},
   "source": [
    "### TASK 1: Implement LIME Local Explanations and SP-LIME for Global Explanations\n",
    "\n",
    "Now on to the implementation LIME explanations. \n",
    "\n",
    "#### Task 1.0: \n",
    "**Identify a Few Images to Explain**\n",
    "\n",
    "The code below will display images from the XAI dataset.\n",
    "- Try changing value of `start` to get a new set of images (there are 10 images for each class; for example, the class happy will be at indexes 10-19)\n",
    "- Search through the images to find at least 4 to explain \n",
    "    - Find classes that you would like to explain, and from each class select 2 images\n",
    "        - one should be a correct prediction  \n",
    "        - and one should be an incorrect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d894a9d-7586-4f04-bc41-95f98b4e4713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# packages needed for the rest of the tasks\n",
    "from skimage import io\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178e008-47b2-4134-936f-003776254d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all images from folder\n",
    "images = [io.imread(f) for f in df_xai.image]\n",
    "\n",
    "# gets labels for ground truch and predictions\n",
    "true_labels = [class_names[idx] for idx in y_xai_true]\n",
    "pred_labels = [class_names[idx] for idx in y_xai_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda78971-e83c-447c-b04e-34521fd823fa",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 70\n",
    "utils.display_nine_images(images, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b8f3d-4fa5-4dde-95fd-960286ea9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enter the Indexes Here ### \n",
    "###############################\n",
    "# you will use this array later in the task\n",
    "img_idxs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7bf58a-aef0-42d8-953b-8a01673801a0",
   "metadata": {},
   "source": [
    "#### Task 1.1\n",
    "Now implement a [LimeTabularExplainer](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular), you can review the [LIME tutorial](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html) for help. Make sure to indicate which feature are categorical features as stored in the `categorical_features` variable, as continuous and categorical features are treated differently with LIME.\n",
    "\n",
    "Note: In the feature names, you will see features with a `_c` and a `_r` at the end.  The `_r` means the intentsity of the action unit (i.e., how strong is it's presence), and the `_c` is a binary feature indicating the presence (value=1), or non-presence (value=0), of an action unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d1e72-80d6-413d-b2a8-671827f8609e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9539b-3db5-4fbd-8740-5dfbfa98c8db",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 1.2\n",
    "Generate LIME explanations the previously identified 4 data instances from the `X_xai` dataset, using the `LimeTabularExplainer` and then plotting the explanations for each data instance (see tutorial mentioned above).  \n",
    "\n",
    "HINT: Before showing an explanation, plot the image using `utils.display_one_image()` utility function.  Use `plt.show()` immediately after calling `utils.display_one_image()` to display the image before the explanation charts.\n",
    "\n",
    "Make sure to print out the **True** and **Predicted** labels for each instance.\n",
    "\n",
    "Try experimenting with different parameters for the explainer and explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664685c-0294-46ee-84fb-9a39ad77fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb60985-2d58-4573-a2fa-be9badbfc5a8",
   "metadata": {},
   "source": [
    "#### Talk 1.3\n",
    "Identify the important Facial Actions Units and compare with the images at [Facial Action Units](https://imotions.com/blog/facial-action-coding-system/).  What insights do these local explanations provide?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e9192-1997-4b5c-bede-f0e954e77730",
   "metadata": {},
   "source": [
    "Write your answer here...\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ba13f-8f14-4d8a-b687-8f8f3ae1dca7",
   "metadata": {},
   "source": [
    "#### Task 1.4\n",
    "\n",
    "Now implement [Submodular Pick](https://lime-ml.readthedocs.io/en/latest/lime.html#lime-submodular-pick-module) instance to generate global explanations and see if it provides you with a more global perspective of how the model makes decisions. You can review the [LIME tutorial](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Submodular%20Pick%20examples.ipynb) for help.\n",
    "\n",
    "Try setting `num_exps_desired` to 16 to try to get 2 examples per class (although this isn't guaranteed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439113bd-01d2-4bcf-b500-3d205cc47181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for submodular pick\n",
    "from lime import submodular_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a06c3-33cd-455a-b258-bc7fc0c63063",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91567957-ed63-4319-9589-200b53d21a24",
   "metadata": {},
   "source": [
    "#### Task 1.5\n",
    "Now plot the explantions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d3cab1-39b0-4885-9d3d-f2ab158d43cc",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad0ac4-b398-42f7-83b8-c8c99f0f75fd",
   "metadata": {},
   "source": [
    "#### Bonus Task\n",
    "Generate a pandas dataframe of the explanations and explore the dataframe to gain more insight into the explanations, see the [LIME tutorial](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Submodular%20Pick%20examples.ipynb) for an example.\n",
    "\n",
    "One idea would be to aggregate the most important features for a given emotion and plot a global importance bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef927e80-4dd7-4002-95cd-b8c5fca9de13",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fce866-1d75-4176-90c8-f34a49d7d5bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 1.6\n",
    "\n",
    "How does LIME Submodular Pick select explanations for a global perspective on the model?\n",
    "\n",
    "Identify important AUs for each of the classes and compare with the images at [Facial Action Units](https://imotions.com/blog/facial-action-coding-system/).  What insights do these explanations provide? Do you now have a better understanding of how the model is working? If not, what is lacking using the LIME approach and/or what could be done differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca0f78-0d6b-4eb3-96f8-ca2b165c2b61",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3666d-420d-4bf4-b933-fa54646f1d37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2:  Local Explations of Facial Expression Recognition with Images\n",
    "\n",
    "In this part, we will generate explanations for the CNN trained using facial images (as described in the notebook introduction).\n",
    "\n",
    "First, let's load the data and the trained models. Then we will evaluate the model peformance, before we start with the explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c5a59-febe-476a-99d3-e7fdbd4e7ff5",
   "metadata": {},
   "source": [
    "Set some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fd431-7494-4db7-a6f9-c070e4be00fc",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "SEED = 12\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 80 \n",
    "NUM_CLASSES = 8\n",
    "CLASS_LABELS = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a57a3-5065-4a20-a40b-9d60f30763a5",
   "metadata": {},
   "source": [
    "### Load Pretrained CNN Model and Setup Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2f6c6-c918-405d-80a5-fb4ae74aa07c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# make sure you've downloaded the models from Moodle (see README instructions for Notebook I)\n",
    "model_path = '../models/affectnet_model_e=60/affectnet_model'\n",
    "\n",
    "# test loading weights\n",
    "model_xai = cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "model_xai.load_weights(model_path).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73340803-65f0-41e7-a800-de56bdec1ad1",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_dir = '../data/affectnet/val_class/'\n",
    "\n",
    "# Load data\n",
    "test_datagen = ImageDataGenerator(validation_split=0.2,\n",
    "                                  rescale=1./255)\n",
    "test_gen = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            color_mode='rgb',\n",
    "                                            class_mode='categorical', \n",
    "                                            seed = SEED)\n",
    "images, classes = next(test_gen) # since batch size is set to 80, this will load the entire test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e29bf-9bbc-4eb3-8fd1-9d9f6c801830",
   "metadata": {},
   "source": [
    "### Evaluation and Predictions\n",
    "Here we evaluate the loaded model to ensure it is working as expected.  You should get around $55\\%$ accuracy. While this is not a perfect classifier is above random guessing which is $1 / 8 * 100 = 12.5$ accuracy\n",
    "\n",
    "Then we load predictions to use throughout the notebook. \n",
    "\n",
    "The predictions results can then be viewed with a confusion matrix to see where the model is confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5d926-6b4a-400b-bf82-4e47c8bdbcd8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "loss, acc = model_xai.evaluate(test_gen, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6532b5-e0b6-4d8f-8227-2e4858763eb1",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# get softmax predictions from model\n",
    "preds = model_xai(images)\n",
    "\n",
    "# convert predictions to integers\n",
    "y_pred = np.argmax(preds, axis=-1)\n",
    "y_true = np.argmax(classes, axis=-1)\n",
    "\n",
    "# print detailed results\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58419a6d-6d1b-4e83-b1b7-6e1625bda5db",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# we can also review the confusion matrix\n",
    "cm_data = confusion_matrix(y_true, y_pred)\n",
    "cm = pd.DataFrame(cm_data, columns=CLASS_LABELS, index = CLASS_LABELS)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ce90b-8453-4ade-a830-3b7402ab4534",
   "metadata": {},
   "source": [
    "### TASK 2: LIME Local Prediction Explanations\n",
    "\n",
    "Now that we have our model setup, we will review the images and predictions to identify a few data instances to explain.  \n",
    "\n",
    "#### Task 2.0\n",
    "\n",
    "Identify a Few Images to Explain\n",
    "\n",
    "The code below will display images from the XAI dataset.\n",
    "\n",
    "- Try changing start value to get a new set of images (there are 10 images for each class, so for example, the class happy will be at indexes 10-19)\n",
    "- Search through the images to find at least 4 to explain \n",
    "    - Find classes that you would like to explain, and from each class select 2 images\n",
    "        - one should be a correct prediction  \n",
    "        - and one should be an incorrect prediction\n",
    "        - These can be the same as previously select (but note that that the indexes are different due to different methods of reading the files from disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd54dad-245d-478d-b2ea-913b9ee193fd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 0\n",
    "\n",
    "true_labels = [CLASS_LABELS[idx] for idx in y_true]\n",
    "pred_labels = [CLASS_LABELS[idx] for idx in y_pred]\n",
    "utils.display_nine_images(images, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e8bf2-6d47-4051-8399-a7e04905e9e2",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#### Enter the Indexes Here ### \n",
    "###############################\n",
    "# you will use this array later in this task\n",
    "img_idxs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbf361-07c1-457d-bb61-20589a6b25e7",
   "metadata": {},
   "source": [
    "#### Task 2.1 \n",
    "**Implement a LIME Image Explainer**\n",
    "\n",
    "Implement a [LimeImageExplainer](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_image) instance, you can review the [LIME tutorial](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb) for help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f1575-52d8-4929-b2fd-55c8adec047b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "\n",
    "from skimage.segmentation import mark_boundaries # used to get boundries from explanation for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b5151-4480-409b-b250-f62439867967",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c2afb",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "#### Task 2.2\n",
    "\n",
    "As we discussed in the seminar, LIME requires images to be segmented into superpixels.  For facial expressions the segmentation algorithm is very important and the default of the `explain_instance()` method may not provide good explanations.  Experiment with different segmenters using the LIME `SegmentationAlgorithm` class and pass to `segmenter_fn` argument of the `explain_instance()` method.\n",
    "\n",
    "- For example: `segmenter = SegmentationAlgorithm('method name', params)`, where params are defined by the skimage segmentation alorithm method\n",
    "- by default LIME uses: \n",
    "```\n",
    "segmenter = SegmentationAlgorithm('quickshift', kernel_size=4,\n",
    "                                  max_dist=200, ratio=0.2,\n",
    "                                  random_seed=random_seed)\n",
    "```\n",
    "\n",
    "You can find different algorithms via [skimage segmentation](https://scikit-image.org/docs/stable/api/skimage.segmentation.html)\n",
    "\n",
    "For this task, implement and visualize the default LIME segmenter and at least one other segementation algorithm, and choose which you think is best for generating a LIME explanation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74a75e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "# segmenter = SegmentationAlgorithm('quickshift', kernel_size=2,\n",
    "#                                   max_dist=10, ratio=0.2,\n",
    "#                                   random_seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e5fe1-8515-410f-a697-623e077d759f",
   "metadata": {},
   "source": [
    "#### Task 2.3\n",
    "\n",
    "Now generate the explanations for each of the selected images. Experiment with the default segmenter and another segmenter as found above in task 2.2a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba91e3b98d59f9",
   "metadata": {
    "collapsed": false,
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea942c-41c2-4e5a-912f-963d82240ed3",
   "metadata": {},
   "source": [
    "#### Task 2.4: \n",
    "\n",
    "**Visualize Explanations**\n",
    "\n",
    "Visualize the explanations for each of the 4 data points from LIME using matplotlib's `imshow` function (see above tutorial). (Or pass the explanation to the `display_one_image` from the `utils` module.)\n",
    "\n",
    "*HINT*: Use the `subplot` parameter of the `display_one_image` to plot a 2x2 grid.  The value should be an integer formated as `RCN` where `R` is the number of rows, `C` is the number of columns, and `C` is the number of the image to plot.  For example, `221` means to plot the first image of a 2x2 grid, `222` means the plot the second images, and so forth... (also see `display_nine_images` for example of this usage.)\n",
    "\n",
    "Experiment with at least 2 different sets of parameters for the explanation visualizations.  For example, view positive and negative contributions, change the number of features for the explation, or try visualizing a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71a9a8-bd68-4a54-b330-41d611ffd55b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421706d-4b9a-4ed2-af6b-fec85dd15f84",
   "metadata": {},
   "source": [
    "#### Task 2.5 \n",
    "**Report on your findings**\n",
    "\n",
    "What are your insights have you gained about the predictions? Can you identify any patterns that explain how the model is working? Are you more or less confident in the model's performance after reviewing the explanations?\n",
    "\n",
    "Also, discuss how you might generate global explanations in this case.  Since SP-LIME doesn't work directly on images, can you think of other ways to generate global explanations from LIME?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73f8b5-b207-46e6-bdce-e5614ad4871f",
   "metadata": {},
   "source": [
    "Write here findings here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62025ef2c1c01e63",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da84a2345945f271",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TASK 3a: Saliency Maps\n",
    "\n",
    "In this task you will create and compare saliency maps using 3 different approaches from the tensorflow based package, [iNNvestigate](https://github.com/albermax/innvestigate). You can review the MNIST example notebook to see how the package works, https://github.com/albermax/innvestigate/blob/master/examples/mnist_compare_methods.ipynb\n",
    "\n",
    "First we need to load and setup the model for use with the package.  Gradient based methods require that we remove the SoftMax layer from the network, and additionally we have to replace the Batch Normalization layers for methods other than Layerwise Relevance Propagation.  This can be done using the `prep_innvestigate_model` function below. For example, for using the model with LRP:\n",
    "\n",
    "```python\n",
    "# if using an innvestigate methods other than LRP set replace_bn=True\n",
    "model_inn = prep_innvestigate_model(model, replace_bn=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0503e16",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import innvestigate as inn\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "tf.compat.v1.disable_eager_execution() # we have to disable eager execution for iNNvestigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Post Processing Utilities\n",
    "############################\n",
    "def postprocess(X):\n",
    "    X = X.copy()\n",
    "    X = iutils.postprocess_images(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def bk_proj(X):\n",
    "    return ivis.graymap(X)\n",
    "\n",
    "\n",
    "def heatmap(X):\n",
    "    # innvestigate heatmap doesn't appear to work well with batch of more than 1 image, \n",
    "    # so this replaces the default\n",
    "    # Aggregate along color channels and normalize to [-1, 1]\n",
    "    exps_ag = X.sum(axis=-1)\n",
    "    exps_ag /= np.max(np.abs(exps_ag), axis=(-2, -1))[:, None, None]\n",
    "    return exps_ag\n",
    "\n",
    "\n",
    "def graymap(X):\n",
    "    return ivis.graymap(np.abs(X), input_is_positive_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_innvestigate_model(model, replace_bn=False):\n",
    "    \"\"\" Prepare model for use with innvestigate package\n",
    "    \n",
    "    Args:\n",
    "        model: tensorflow model to be explained\n",
    "        replace_bn (bool): should the batch normalization layers be replace with standard normalization. \n",
    "                           This is needed for most innvestigate methods except LRP\n",
    "    Returns:\n",
    "        tensorflow model that is compatible with innvestigate\n",
    "    \"\"\"\n",
    "    model_tmp = model\n",
    "    # innvestigate doesn't support batch norm for some models\n",
    "    # so we much substitute BN layers with manual normalization layers\n",
    "    # https://github.com/albermax/innvestigate/issues/292\n",
    "    if replace_bn:\n",
    "        bn_layers = [2, 6, 10, 14, 19, 22]\n",
    "        input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "        layers = model.layers\n",
    "        for idx in bn_layers:\n",
    "            layers = create_bn_replacment(layers, idx)\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for l in layers:\n",
    "            x = l(x)\n",
    "        outputs = x\n",
    "        model_tmp = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # remove softmax for gradient computations and return model\n",
    "    return inn.model_wo_softmax(model_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705540d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "# reload model since we disabled eager execution\n",
    "model = cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "model.load_weights(model_path).expect_partial()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90a19e",
   "metadata": {},
   "source": [
    "#### Task 3a.1\n",
    "\n",
    "In this task we will setup at least three different explainers, make sure one is an LRP implementation. \n",
    "\n",
    "- To do this first, setup a dictionary for your explainers and their parameters. Then create a list of analyzers initialized from the dictionary of methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a39511",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc129f0d",
   "metadata": {},
   "source": [
    "#### Task 3a.2\n",
    "\n",
    "Now generate the explanations for your four selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c04bf",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1d900",
   "metadata": {},
   "source": [
    "#### Task 3a.3\n",
    "\n",
    "Use matplotlib to generate a grid of explanation similar to the MNIST example.  Each row should be an image to be explained, and each column is one of the specific methods.  Make sure to also print somewhere the image ground truth and model prediction. These should be stored in the variables `true_labels` and `pred_labels` from Task 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625eab6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 3a.4\n",
    "\n",
    "How do the methods compare? Do find that one is better than the other for explaining the predicition of the model?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b21dd371ccf7a97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Type your answer here..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df4d6aaace5723a4"
  },
  {
   "cell_type": "markdown",
   "id": "85a1d34e-1141-4f16-a0be-131a3d4f7a0c",
   "metadata": {},
   "source": [
    "### TASK 3b: Grad-CAM\n",
    "\n",
    "We don't discuss GradCAM (Gradient-weighted Class Activation Maps) in the seminar this semester, but it is a commonly used approach for generating saliency maps. While it is a backpropagation based method, it takes a slightly different approach than other methods.  Instead, of backpropagating all the way to the input pixels, GradCAM calculates gradients only to final convolutional layers.  The gradient values (ie importance scores) are then used to linearly combined the activation maps from the last layer by weighting each map by the sum of the gradients for that map. [See the orignal paper for full details](https://arxiv.org/abs/1610.02391)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176599cf-57bd-415e-9fd2-053a66c00f04",
   "metadata": {},
   "source": [
    "#### Task 3b.1\n",
    "**Implement the GradCAM Algorithm**\n",
    "\n",
    "For this task  you will implement a version of GradCAM based on the [Keras GradCAM Tutorial](https://keras.io/examples/vision/grad_cam/). To better understand the algorithm, rather than just copying and pasting the entire functions, try implementing each main step of the alorithm in a seperate cell.  Then review the output of that cell either by printing the tensor or tensor shape.  Or by visualizing the output with matplotlib, for example with `plt.imshow()`\n",
    "\n",
    "**Note:**  the final convolution layer in our network is named `final_conv_layer`\n",
    "\n",
    "**Note 2:** if you've also completed Task 3a, you will need to restart your notebook. Then rerun without enabling eagar execution in the import cell of Task 3a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reload model\n",
    "model_gcam = cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "model_gcam.load_weights(model_path).expect_partial()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68df2b53febeefb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196852c4-6664-40fb-af15-4ac18c0d70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be98ae-e7f6-4d7e-80d3-2faa0476baf9",
   "metadata": {},
   "source": [
    "#### Task 3b.2\n",
    "\n",
    "Wrap the GradCAM Implementation into functions for generating the heatmaps and creating a superimposed image.  Instead of saving the superimposed image to a file (as done in the tutorial), simply generate the superimposed image and return it. We will use it in the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b176bf-0d1a-4c9f-99dc-f820d865cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee759c8-6d3f-4d87-b8f2-dfbbfbd3eed5",
   "metadata": {},
   "source": [
    "#### Task 3b.3\n",
    "\n",
    "Using your new GradCAM functions and the `display_one_image()` function in the `utils` model display the heatmaps for your images.  However, instead of just displaying the heatmaps for the top predicated class, for each of your images generate a heat map for each class label, see the file `gradcam_example.png` for an example of what this might look like. Also, for each image, add a border to the image representing the predicted class.  See the docstring of `display_one_image()` for details on usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a67f89-67bf-468d-a551-3c99fca35d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n",
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a322229-6d56-411d-a4e2-8a4fb6c36f23",
   "metadata": {},
   "source": [
    "#### Task 3b.5\n",
    "\n",
    "What patterns do you notice in the heatmaps for the different classes. Do the regions of the image where the model focuses make sense? Or do you notice any unusual biases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88389d57-9fac-423b-bdc7-50eede30dae4",
   "metadata": {},
   "source": [
    "### TASK 4: Final Discussion\n",
    "Between tasks 1, 2, and 3, which of the 2 methods best help you understand how the model is function, as we've discussed throughout the seminar. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730cce4-3b0a-4709-836e-15fe01acebe5",
   "metadata": {},
   "source": [
    "write your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fed70-2112-406a-81ad-63e23c0d9d42",
   "metadata": {},
   "source": [
    "### BONUS TASK\n",
    "\n",
    "Try plot LIME output and GradCAM outpt for the same images next too each other.  Are the results similar or are there difference in indentified important regions?\n",
    "\n",
    "Or feel free to generate your own plots or visualizations to help understand the explanations and models better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70429e-b6c9-40c7-8abd-3b9dbaa001e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
